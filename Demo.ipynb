{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network from Kell, Yamins, Shook, Norman-Haignere, McDermott, 2018\n",
    "\n",
    "This notebook shows how to create a tensorflow graph for the network with the weights and biases used in <a href=\"https://www.cell.com/neuron/fulltext/S0896-6273(18)30250-2\">Kell et al., 2018</a>. This notebook also gives an example of how to pass a sound into the network.\n",
    "\n",
    "\n",
    "### Note on network input\n",
    "\n",
    "The input to the network is a \"cochleagram\", a time-frequency decomposition of a sound that is similar to a spectrogram. Below we provide examples of how to pass a pre-computed cochleagram into the network, as well as how to compute the cochleagram for an example wav and then pass that cochleagram to the network.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "Most of the dependencies to run this are relatively standard. However, please note the following:\n",
    "- This notebook was tested and run with version 1.5.0 of `tensorflow`. It was not tested with other versions.\n",
    "- `pycochleagram` is a module to generate cochleagrams to pass sounds into the network, which can be found <a href=\"https://github.com/mcdermottLab/pycochleagram\">here</a>.\n",
    "- `PIL` is the Python Image Library.\n",
    "\n",
    "### Contact\n",
    "If you have any questions, please contact Alex Kell. Email: < first_name >< last_name >@mit.edu.\n",
    "\n",
    "Thanks, and enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import branched_network_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import sys\n",
    "sys.path.append('./network/')\n",
    "from branched_network_class import branched_network\n",
    "import tensorflow as tf\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the following to run demo_from_wav()\n",
    "\n",
    "from cochgram.pycochleagram import cochleagram as cgram\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Some helper functions\n",
    "def resample(example, new_size):\n",
    "    im = Image.fromarray(example)\n",
    "    resized_image = im.resize(new_size, resample=Image.ANTIALIAS)\n",
    "    return np.array(resized_image)\n",
    "\n",
    "def plot_cochleagram(cochleagram, title): \n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.matshow(cochleagram.reshape(256,256), origin='lower',cmap=plt.cm.Blues, fignum=False, aspect='auto')\n",
    "    plt.yticks([]); plt.xticks([]); plt.title(title); \n",
    "    \n",
    "def play_wav(wav_f, sr, title):   \n",
    "    print (title+':')\n",
    "    ipd.display(ipd.Audio(wav_f, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demo_pre_generated_cochleagram():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    net_object = branched_network() # make network object\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy') #Load logits to word key \n",
    "    music_key = np.load('./demo_stim/logits_to_genre_key.npy') #Load logits to genre key\n",
    "\n",
    "    # example pre-generated speech cochleagram \n",
    "    example_cochleagram = np.load('./demo_stim/example_cochleagram_0.npy') \n",
    "    plot_cochleagram(example_cochleagram,'Example speech cochleagram' )\n",
    "\n",
    "    # run cochleagram through network and get logits for word branch\n",
    "    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: example_cochleagram})\n",
    "\n",
    "    # determine word branch prediction \n",
    "    prediction = word_key[np.argmax(logits, axis=1)]\n",
    "    print \"Speech Example ... actual label: according  predicted_label: \" + prediction[0] +'\\n'\n",
    "    \n",
    "    # example pre-generated music cochleagram\n",
    "    example_cochleagram_music = np.load('./demo_stim/example_cochleagram_1.npy') \n",
    "    plot_cochleagram(example_cochleagram_music,'Example music cochleagram' )\n",
    "    \n",
    "    # run cochleagram through network and get logits for genre branch\n",
    "    logits_music = net_object.session.run(net_object.genre_logits, \n",
    "                                          feed_dict={net_object.x: example_cochleagram_music})\n",
    "    # note: throughout paper top-5 accuracy is reported for genre task\n",
    "    prediction_music = (logits_music).argsort()[:,-5:][0][::-1] \n",
    "    print \"Music Example... actual label: \"+ music_key[11]+ \"  top-5 predicted_labels (in order of confidence): \"\n",
    "    print \"\\n\"+ \"; \".join(music_key[prediction_music])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_cochleagram(wav_f, sr, title):\n",
    "    # define parameters\n",
    "    n, sampling_rate = 50, 16000\n",
    "    low_lim, hi_lim = 20, 8000\n",
    "    sample_factor, pad_factor, downsample = 4, 2, 200\n",
    "    nonlinearity, fft_mode, ret_mode = 'power', 'auto', 'envs'\n",
    "    strict = True\n",
    "\n",
    "    # create cochleagram\n",
    "    c_gram = cgram.cochleagram(wav_f, sr, n, low_lim, hi_lim, \n",
    "                               sample_factor, pad_factor, downsample,\n",
    "                               nonlinearity, fft_mode, ret_mode, strict)\n",
    "    \n",
    "    # rescale to [0,255]\n",
    "    c_gram_rescaled =  255*(1-((np.max(c_gram)-c_gram)/np.ptp(c_gram)))\n",
    "    \n",
    "    # reshape to (256,256)\n",
    "    c_gram_reshape_1 = np.reshape(c_gram_rescaled, (211,400))\n",
    "    c_gram_reshape_2 = resample(c_gram_reshape_1,(256,256))\n",
    "    \n",
    "    plot_cochleagram(c_gram_reshape_2, title)\n",
    "\n",
    "    # prepare to run through network -- i.e., flatten it\n",
    "    c_gram_flatten = np.reshape(c_gram_reshape_2, (1, 256*256)) \n",
    "    \n",
    "    return c_gram_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demo_from_wav():\n",
    "    tf.reset_default_graph()\n",
    "    net_object = branched_network()\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy') # load logits to word key\n",
    "    music_key = np.load('./demo_stim/logits_to_genre_key.npy') # load logits to word key \n",
    "    \n",
    "    \n",
    "    # generate cochleagram, then pass cochleagram through network and get logits for word branch\n",
    "    \n",
    "    ## Speech examples\n",
    "    \n",
    "    # example 1:\n",
    "    sr, wav_f = wav.read('./demo_stim/example_1.wav') # note the sampling rate is 16000hz.\n",
    "    play_wav(wav_f, sr, 'Example 1')\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example 1')\n",
    "    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = word_key[np.argmax(logits, axis=1)]\n",
    "    print \"Speech Example ... \\n clean speech, actual label: Increasingly, predicted_label: \" \\\n",
    "        + prediction[0] +'\\n'\n",
    "    \n",
    "    # example 2:\n",
    "    sr, wav_f = wav.read('./demo_stim/example_2.wav') \n",
    "    play_wav(wav_f, sr, 'Example 2')\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example 2')\n",
    "    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = word_key[np.argmax(logits, axis=1)]\n",
    "    print \"Speech Example ... \\n background: speaker shaped noise, snr: -3db, actual label: Washington, predicted_label: \" \\\n",
    "        + prediction[0] +'\\n'\n",
    "    \n",
    "    # example 3:\n",
    "    sr, wav_f = wav.read('./demo_stim/example_3.wav') \n",
    "    play_wav(wav_f, sr, 'Example 3')\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example 3')\n",
    "    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = word_key[np.argmax(logits, axis=1)]\n",
    "    print \"Speech Example ... \\n background: music, snr: 3db, actual label: Movies, predicted_label: \" \\\n",
    "        + prediction[0] +'\\n'\n",
    "    \n",
    "     \n",
    "    # example 4:\n",
    "    sr, wav_f = wav.read('./demo_stim/example_4.wav') \n",
    "    play_wav(wav_f, sr, 'Example 4')\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example 4')\n",
    "    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = word_key[np.argmax(logits, axis=1)]\n",
    "    print \"Speech Example ... \\n background: 2-spkr babble, snr: -6db, actual label: Equipment  predicted_label: \" \\\n",
    "        + prediction[0] +'\\n'\n",
    "    \n",
    "    # example 5:\n",
    "    sr, wav_f = wav.read('./demo_stim/example_5.wav') \n",
    "    play_wav(wav_f, sr, 'Example 5')\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example 5')\n",
    "    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = word_key[np.argmax(logits, axis=1)]\n",
    "    print \"Speech Example ... \\n background: Auditory scene, snr: -9db, actual label: Advantage  predicted_label: \" \\\n",
    "        + prediction[0] +'\\n'\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Music examples \n",
    "    \n",
    "    # example 6:\n",
    "    sr, wav_f = wav.read('./demo_stim/example_6.wav') \n",
    "    play_wav(wav_f, sr, 'Example 6')\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example 6')\n",
    "    logits = net_object.session.run(net_object.genre_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = (logits).argsort()[:,-5:][0][::-1] \n",
    "    print \"Music Example ... \\n Background: Auditory Scene, snr: -3db, actual label: \" \\\n",
    "        + music_key[1] + \",\\n top-5 predicted_labels (in order of confidence): \\n \" \\\n",
    "        + \";\\n \".join(music_key[prediction]) + \"\\n\"\n",
    "    \n",
    "    # example 7:\n",
    "    sr, wav_f = wav.read('./demo_stim/example_7.wav') \n",
    "    play_wav(wav_f, sr, 'Example 7')\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example 7')\n",
    "    logits = net_object.session.run(net_object.genre_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = (logits).argsort()[:,-5:][0][::-1] \n",
    "    print \"Music Example ... \\n Background: Music-shaped noise, snr: -6db, actual label: \" \\\n",
    "        + music_key[10] + \",\\n top-5 predicted_labels (in order of confidence): \\n \" \\\n",
    "        + \";\\n \".join(music_key[prediction]) + \"\\n\"\n",
    "    \n",
    "    # example 8:\n",
    "    sr, wav_f = wav.read('./demo_stim/example_8.wav') \n",
    "    play_wav(wav_f, sr, 'Example 8')\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example 8')\n",
    "    logits = net_object.session.run(net_object.genre_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = (logits).argsort()[:,-5:][0][::-1] \n",
    "    print \"Music Example ... \\n Background: 8-spkr babble, snr: 3db, actual label: \" \\\n",
    "        + music_key[33] + \",\\n top-5 predicted_labels (in order of confidence): \\n \" \\\n",
    "        + \";\\n \".join(music_key[prediction]) + \"\\n\"\n",
    "    \n",
    "    # example 9:\n",
    "    sr, wav_f = wav.read('./demo_stim/example_9.wav') # note the sampling rate is 16000hz.\n",
    "    play_wav(wav_f, sr, 'Example 9')\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example 9')\n",
    "    logits = net_object.session.run(net_object.genre_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = (logits).argsort()[:,-5:][0][::-1] \n",
    "    print \"Music Example ... \\n Background: 2-spkr babble, snr: 0db, actual label: \" \\\n",
    "        + music_key[39] + \",\\n top-5 predicted_labels (in order of confidence): \\n \" \\\n",
    "        + \";\\n \".join(music_key[prediction]) + \"\\n\"\n",
    "    \n",
    "    # example 10:\n",
    "    sr, wav_f = wav.read('./demo_stim/example_10.wav') # note the sampling rate is 16000hz.\n",
    "    play_wav(wav_f, sr, 'Example 10')\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example 10')\n",
    "    logits = net_object.session.run(net_object.genre_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = (logits).argsort()[:,-5:][0][::-1] \n",
    "    print \"Music Example ... \\n Background: clean, actual label: \" \\\n",
    "        + music_key[16] + \",\\n top-5 predicted_labels (in order of confidence): \\n \" \\\n",
    "        + \";\\n \".join(music_key[prediction]) + \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_pre_generated_cochleagram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "demo_from_wav()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
